{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92561f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40793d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function choice:\n",
      "\n",
      "choice(...) method of numpy.random.mtrand.RandomState instance\n",
      "    choice(a, size=None, replace=True, p=None)\n",
      "    \n",
      "    Generates a random sample from a given 1-D array\n",
      "    \n",
      "    .. versionadded:: 1.7.0\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``choice`` method of a ``default_rng()``\n",
      "        instance instead; please see the :ref:`random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : 1-D array-like or int\n",
      "        If an ndarray, a random sample is generated from its elements.\n",
      "        If an int, the random sample is generated as if it were ``np.arange(a)``\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
      "        single value is returned.\n",
      "    replace : boolean, optional\n",
      "        Whether the sample is with or without replacement. Default is True,\n",
      "        meaning that a value of ``a`` can be selected multiple times.\n",
      "    p : 1-D array-like, optional\n",
      "        The probabilities associated with each entry in a.\n",
      "        If not given, the sample assumes a uniform distribution over all\n",
      "        entries in ``a``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    samples : single item or ndarray\n",
      "        The generated random samples\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If a is an int and less than zero, if a or p are not 1-dimensional,\n",
      "        if a is an array-like of size 0, if p is not a vector of\n",
      "        probabilities, if a and p have different lengths, or if\n",
      "        replace=False and the sample size is greater than the population\n",
      "        size\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    randint, shuffle, permutation\n",
      "    random.Generator.choice: which should be used in new code\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Setting user-specified probabilities through ``p`` uses a more general but less\n",
      "    efficient sampler than the default. The general sampler produces a different sample\n",
      "    than the optimized sampler even if each element of ``p`` is 1 / len(a).\n",
      "    \n",
      "    Sampling random rows from a 2-D array is not possible with this function,\n",
      "    but is possible with `Generator.choice` through its ``axis`` keyword.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Generate a uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3)\n",
      "    array([0, 3, 4]) # random\n",
      "    >>> #This is equivalent to np.random.randint(0,5,3)\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([3, 3, 0]) # random\n",
      "    \n",
      "    Generate a uniform random sample from np.arange(5) of size 3 without\n",
      "    replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False)\n",
      "    array([3,1,0]) # random\n",
      "    >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size\n",
      "    3 without replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([2, 3, 0]) # random\n",
      "    \n",
      "    Any of the above can be repeated with an arbitrary array-like\n",
      "    instead of just integers. For instance:\n",
      "    \n",
      "    >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
      "    >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\n",
      "    array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n",
      "          dtype='<U11')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dbbf5",
   "metadata": {},
   "source": [
    "## Sampling Novel Sequences (video 7)\n",
    "\n",
    "What is the purpose of sampling novel sequences?\n",
    "Oh, it's like having the model learn Shakespear and then having it generate (\"sample\") sentences (\"sequences\")\n",
    "That is, \"sampling from the model that you've trained\"\n",
    "\n",
    "One advantage of character-level language model: you will not encounter unknown elements in a sequence\n",
    "But they are more computationally expensive to train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8049c7",
   "metadata": {},
   "source": [
    "## Vanishing Gradient with RNNs (video 8)\n",
    "\n",
    "A problem with RNNs. Why?\n",
    "Long-term dependencies; e.g., \"The *cat* ... *was* full\", \"The *cats* ... *were* full\", where the later word, in both cases depends on whether or not the earlier word (cat) was pluralized (so we have to go *way* back in the sequence to find out)\n",
    "\n",
    "Recall **vanishing gradient problem**: the gradient has a hard time propagating back to change the weight of early layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba438b2",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU) (video 9)\n",
    "\n",
    "Modification to RNN hidden layer that helps with the vanishing gradient problem.\n",
    "\n",
    "Typical hidden layer:\n",
    "\n",
    "$$a^{\\langle t \\rangle} = g(W$$\n",
    "\n",
    "GRU unit has new variable, c, the **memory cell**. $c^{\\langle t \\rangle} = a^{\\langle t \\rangle}$ (for GRUs, the memory cell is equal to the activation)\n",
    "\n",
    "Then $\\tilde{c} = \\tanh \\dots$ is a candidate for replacing c. Decided by $\\Gamma_u \\in \\{0,1\\}$ (Gamma is another \"shape\" analogy, like the Rho algorithms -- think of a gated fence) (\"u\" for update)\n",
    "e.g., $\\Gamma_u$ could decide to update (from 1 to 0, say) if the subject changes from singular to plural. This way, elements of the sequences farther down the sequence can reference the $\\Gamma_u$ rather than going all the way back in the sequence.\n",
    "\n",
    "Note that c is a vector (as are $\\Gamma$ and $\\tilde{c}$, all of the same dimension)\n",
    "\n",
    "3 main equations\n",
    "\n",
    "$\\begin{align}\n",
    "\\tilde{c}^{\\langle t \\rangle} &=\\\\\n",
    "\\Gamma_u &=\\\\\n",
    "c^{\\langle t \\rangle} &= \n",
    "\\end{align}$\n",
    "\n",
    "But wouldn't we need one c per subject (i.e. sentence)?\n",
    "\n",
    "Note: $\\Gamma$ is the **gate** of the **Gated** Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee9930",
   "metadata": {},
   "source": [
    "## 10: Long Short Term Memory (LSTM)\n",
    "\n",
    "Alternative to GRU units; that is, another way to learn long-range connections within a sequence.\n",
    "\n",
    "We still have a memory cell, but the candidate is defined differently (note the a^{t-1} in the defining equation. More importantly, **the update gate is different**; namely, there are three gates instead of two, \"update\" gate , \"forget\" gate, and \"output\" gates\n",
    "\n",
    "Long short term memory unit vs GRU unit:\n",
    "- LSTM is more powerful\n",
    "- No widespread consensus on which is better\n",
    "- LSTM units actually came first (see the 1997 paper)\n",
    "- GRU units are simpler\n",
    "    - scales better and less computation heavy\n",
    "- neither is universally superior\n",
    "- LSTMs are more flexible (three gates helps this)\n",
    "- In general, LSTMs are more likely to be the default choice\n",
    "- GRUs have been gaining momentum\n",
    "\n",
    "Common variations:\n",
    "- have gates depend on a^{t-1} as well as c^{t-1} (\"peephole connection\")\n",
    "- \n",
    "\n",
    "Whole purpose of GRUs and LSTMs: capturing long-range dependencies within sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4effd8c",
   "metadata": {},
   "source": [
    "## 11: Bidirectional RNN\n",
    "\n",
    "\"Getting information from the future\" What a cool way to put it.\n",
    "\n",
    "Every RNN that we've seen so far (even those using GRUs or LSTMs) have been forward direction only\n",
    "\n",
    "BRNNs define an acyclic graph\n",
    "\n",
    "Note this all takes place in forwardprop, even though we are going in both directions\n",
    "\n",
    "But how is it implemented? is the order opposide for the reversed green arrow a's in Ng's drawing? i.e. is a^<1> really a^<T_x>?\n",
    "\n",
    "What is the implementation?? I need to see it. (Check Aggarwal or Chollet (10.4.3))\n",
    "\n",
    "Disadvantage: You need the entire sequence of data before running the system; that is, you cannot, e.g., translate \"word-by-word\" as someone is speaking -- only when the entire sentence has been spoken.\n",
    "\n",
    "\n",
    "<details> test </details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518f8c3",
   "metadata": {},
   "source": [
    "## 12: Deep RNNs\n",
    "\n",
    "Stacking layers to the architectures introduced above\n",
    "\n",
    "Oh! What we've been doing up to now has only been done on a single layer. Didn't get that until now.\n",
    "\n",
    "So the RNNs are not communicating between layers, but between what? The neurons in a layer, I think? \n",
    "\n",
    "Because of the temporal dimension (communication between neurons on the same layer), RNNs are generally not as deep as typical feed-forward neural networks.\n",
    "\n",
    "Can have blocks that are GRUs, LSTMs, or BRNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbd2e0",
   "metadata": {},
   "source": [
    "## Quiz Week 1\n",
    "\n",
    "Attempt:\n",
    "\n",
    "1. number in langle denotes position in sequence (\"word\"), number in parentheses denotes training example number\n",
    "2. Judging from the slides, the architecture diagram is not appropriate when Tx>Ty, but I don't see why it wouldn't be if we just set the missing inputs equal to zero, isn't that one thing we discussed in the lectures?\n",
    "3. Classification will always have a single output, so image classification and sentiment classification are many-to-one models <font color='red'>Image classification is an example of one-to-one architecture</font>\n",
    "4. At time t we are calculating the probability of $y^{<t>}$, given that we know $y^{<t-1>{,...,y^{<1>}$\n",
    "5. Confused about this one and how sampling novel sentences works. I think that it is true, but I cannot justify my belief. <font color='red'>False. The probabilities output by the RNN are not used to pick the highest probability word and the ground-truth word from the training set is not the input to the next time-step.</font>\n",
    "6. I do not know how to answer this. If you find that your weights and activations are taking NaN values, then it could be caused by a vanishing gradient (as specified in lecture, this is a prevalent problem in RNNs), but it might also be due to other factors, right? <font color='red'>Yeah, shoulda answered \"False\"...</font>\n",
    "7. As I understood it, in an LSTM unit $\\Gamma_u$ has the same size as $c^{<t>}$, which has the same size as $a^{<t>}$, or am I confusing this with GRU units? Is it not the same for both?\n",
    "8. I need to understand this question better. First of all, I didn't even know about the second gate, $\\Gamma_r$ in a GRU unit. For now I will guess that removing $\\Gamma_r$ will not cause vanishing gradient problems, but I really cannot justify this.\n",
    "9. Is this as simple as it looks? The update gate plays similar roles in both GRU and LSTM, and the forget gate in LSTM plays a role similar to $1 - \\Gamma_u$ in GRU\n",
    "10. Unidirectional is better, for the reason stated (we only care about the weather from the past -- nevermind that we can't get the weather of the future as data)\n",
    "\n",
    "70% \n",
    "\n",
    "Notes on the problems:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71700b",
   "metadata": {},
   "source": [
    "Extra resources:\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f02c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
